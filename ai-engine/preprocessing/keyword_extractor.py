import re
import sys
from typing import List, Dict, Any
from collections import Counter
import spacy

# Try to import KoNLPy, but handle the case where it fails
try:
    from konlpy.tag import Okt, Komoran, Hannanum
    # ì‹¤ì œë¡œ ì´ˆê¸°í™”ë¥¼ ì‹œë„í•´ë³´ê¸°
    test_okt = Okt()
    KONLPY_AVAILABLE = True
except Exception as e:
    print(f"Warning: KoNLPy not available: {e}")
    print("Korean NLP functionality will be disabled. Only English processing will work.")
    KONLPY_AVAILABLE = False
    # Create dummy classes for when KoNLPy is not available
    class Okt:
        def pos(self, text):
            return []
    
    class Komoran:
        def pos(self, text):
            return []
    
    class Hannanum:
        def pos(self, text):
            return []

class KeywordExtractor:
    """
    ë¬¸ì¥ ê¸¸ì´ì— ë”°ë¼ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, lang: str = "ko"):
        self.lang = lang
        self.nlp = None
        self.ko_taggers = {}
        
        # í•œêµ­ì–´ íƒœê±° ì´ˆê¸°í™” (ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨)
        if lang == "ko":
            self._init_korean_taggers()
        
        # ì˜ì–´ spaCy ëª¨ë¸ ì´ˆê¸°í™”
        if lang == "en":
            self._init_english_model()
    
    def _init_korean_taggers(self):
        """í•œêµ­ì–´ íƒœê±°ë“¤ì„ ì•ˆì „í•˜ê²Œ ì´ˆê¸°í™”"""
        try:
            self.ko_taggers['okt'] = Okt()
            print("âœ… Okt íƒœê±° ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸  Okt íƒœê±° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ Javaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”: brew install openjdk@11")
        
        try:
            self.ko_taggers['komoran'] = Komoran()
            print("âœ… Komoran íƒœê±° ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸  Komoran íƒœê±° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        try:
            self.ko_taggers['hannanum'] = Hannanum()
            print("âœ… Hannanum íƒœê±° ì´ˆê¸°í™” ì„±ê³µ")
        except Exception as e:
            print(f"âš ï¸  Hannanum íƒœê±° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    def _init_english_model(self):
        """ì˜ì–´ spaCy ëª¨ë¸ ì´ˆê¸°í™”"""
        try:
            self.nlp = spacy.load("en_core_web_sm")
            print("âœ… spaCy ì˜ì–´ ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except OSError:
            print("âŒ spaCy ì˜ì–´ ëª¨ë¸ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”: python -m spacy download en_core_web_sm")
        except Exception as e:
            print(f"âŒ spaCy ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    def extract_keywords(self, text: str, method: str = "okt") -> Dict[str, Any]:
        """
        ë¬¸ì¥ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ.
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            method: í•œêµ­ì–´ íƒœê±° ë°©ë²• ("okt", "komoran", "hannanum")
        
        Returns:
            í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if not text or not text.strip():
            return {
                "error": "ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.",
                "original_text": text,
                "language": self.lang
            }
        
        if self.lang == "ko":
            if not KONLPY_AVAILABLE or not self.ko_taggers:
                return {
                    "error": "KoNLPyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í•œêµ­ì–´ ì²˜ë¦¬ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.",
                    "original_text": text,
                    "language": "ko"
                }
            return self._extract_keywords_ko(text, method)
        else:
            return self._extract_keywords_en(text)
    
    def _extract_keywords_ko(self, text: str, method: str = "okt") -> Dict[str, Any]:
        """
        í•œêµ­ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
        """
        # ì‚¬ìš© ê°€ëŠ¥í•œ íƒœê±° í™•ì¸
        if not self.ko_taggers:
            return {
                "error": "ì‚¬ìš© ê°€ëŠ¥í•œ í•œêµ­ì–´ íƒœê±°ê°€ ì—†ìŠµë‹ˆë‹¤. Java ì„¤ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.",
                "original_text": text,
                "language": "ko"
            }
        
        # ì„ íƒëœ íƒœê±°ê°€ ì‚¬ìš© ê°€ëŠ¥í•œì§€ í™•ì¸
        if method not in self.ko_taggers:
            available_methods = list(self.ko_taggers.keys())
            return {
                "error": f"'{method}' íƒœê±°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ íƒœê±°: {available_methods}",
                "original_text": text,
                "language": "ko",
                "available_methods": available_methods
            }
        
        try:
            # ë¬¸ì¥ ì •ì œ
            cleaned_text = self._clean_text(text)
            words = cleaned_text.split()
            word_count = len(words)
            
            # í˜•íƒœì†Œ ë¶„ì„
            tagger = self.ko_taggers[method]
            pos_tags = tagger.pos(cleaned_text)
            
            # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ
            keywords = self._extract_all_words_ko(pos_tags, word_count)
            keyword_count = len(keywords)
            
            return {
                "original_text": text,
                "cleaned_text": cleaned_text,
                "word_count": word_count,
                "keyword_count": keyword_count,
                "keywords": keywords,
                "method": method,
                "language": "ko"
            }
        except Exception as e:
            return {
                "error": f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "original_text": text,
                "language": "ko",
                "method": method
            }
    
    def _extract_keywords_en(self, text: str) -> Dict[str, Any]:
        """
        ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ
        """
        if not self.nlp:
            return {
                "error": "spaCy ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'python -m spacy download en_core_web_sm' ì‹¤í–‰ í•„ìš”",
                "original_text": text,
                "language": "en"
            }
        
        try:
            # ë¬¸ì¥ ì •ì œ
            cleaned_text = self._clean_text(text)
            doc = self.nlp(cleaned_text)
            word_count = len([token for token in doc if not token.is_space])
            
            # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ
            keywords = self._extract_all_words_en(doc, word_count)
            keyword_count = len(keywords)
            
            return {
                "original_text": text,
                "cleaned_text": cleaned_text,
                "word_count": word_count,
                "keyword_count": keyword_count,
                "keywords": keywords,
                "language": "en"
            }
        except Exception as e:
            return {
                "error": f"ì˜ì–´ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "original_text": text,
                "language": "en"
            }
    
    def _extract_all_words_ko(self, pos_tags: List[tuple], keyword_count: int) -> List[Dict[str, str]]:
        """
        í•œêµ­ì–´ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ (ì¡°ì‚¬, ì ‘ì†ì‚¬ ë“± ì œì™¸)
        """
        # ì œì™¸í•  í’ˆì‚¬ë“¤
        exclude_pos = [
            'JKS', 'JKC', 'JKG', 'JKO', 'JKB', 'JKV', 'JKQ', 'JX', 'JC',  # ì¡°ì‚¬
            'EP', 'EF', 'EC', 'ETN', 'ETM',  # ì–´ë¯¸
            'XSN', 'XSV', 'XSA', 'XSM',  # ì ‘ë¯¸ì‚¬
            'SF', 'SP', 'SS', 'SE', 'SO', 'SW',  # ê¸°í˜¸
            'UN', 'UV', 'UE',  # ë¯¸ë¶„ì„
        ]
        
        keywords = []
        for word, pos in pos_tags:
            if len(word) > 0 and pos not in exclude_pos:
                keywords.append({
                    'word': word,
                    'pos': pos
                })
        
        return keywords[:keyword_count]
    
    def _extract_all_words_en(self, doc, keyword_count: int) -> List[Dict[str, str]]:
        """
        ì˜ì–´ ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë§Œ ì¶”ì¶œ (ê´€ì‚¬, ì ‘ì†ì‚¬ ë“± ì œì™¸)
        """
        # ì œì™¸í•  í’ˆì‚¬ë“¤
        exclude_pos = [
            'DET',  # ê´€ì‚¬ (a, an, the)
            'CCONJ',  # ì ‘ì†ì‚¬ (and, or, but)
            'SCONJ',  # ì¢…ì†ì ‘ì†ì‚¬ (if, because)
            'AUX',  # ë³´ì¡°ë™ì‚¬ (is, are, have)
            'PART',  # ë¶„ì‚¬ (to)
            'PUNCT',  # êµ¬ë‘ì 
            'SPACE',  # ê³µë°±
        ]
        
        keywords = []
        for token in doc:
            if (not token.is_space and 
                len(token.text) > 0 and 
                token.pos_ not in exclude_pos and
                not token.is_stop):  # ë¶ˆìš©ì–´ë„ ì œì™¸
                keywords.append({
                    'word': token.text,
                    'pos': token.pos_
                })
        
        return keywords[:keyword_count]
    
    def _clean_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ì •ì œ
        """
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (ë§ˆì¹¨í‘œ, ì‰¼í‘œ, ë¬¼ìŒí‘œ, ëŠë‚Œí‘œëŠ” ìœ ì§€)
        text = re.sub(r'[^\w\s\.\,\?\!]', '', text)
        # ì—°ì†ëœ ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text)
        return text.strip()

def extract_keywords(text: str, lang: str = "ko", method: str = "okt") -> Dict[str, Any]:
    """
    í‚¤ì›Œë“œ ì¶”ì¶œ í†µí•© í•¨ìˆ˜
    
    Args:
        text: ë¶„ì„í•  í…ìŠ¤íŠ¸
        lang: ì–¸ì–´ ("ko" ë˜ëŠ” "en")
        method: í•œêµ­ì–´ íƒœê±° ë°©ë²• ("okt", "komoran", "hannanum")
    
    Returns:
        í‚¤ì›Œë“œ ì¶”ì¶œ ê²°ê³¼
    """
    extractor = KeywordExtractor(lang)
    return extractor.extract_keywords(text, method)

if __name__ == "__main__":
    print("ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        {
            "text": "ì„¸ì¢…ëŒ€ì™•ì€ 1392ë…„ì— ì¡°ì„ ì„ ê±´êµ­í–ˆë‹¤.",
            "lang": "ko",
            "method": "okt"
        },
        {
            "text": "ë‚ ì”¨ê°€ ì¢‹ë‹¤.",
            "lang": "ko", 
            "method": "okt"
        },
        {
            "text": "ì•„ì´ê°€ ê³µì›ì—ì„œ ì¹œêµ¬ë“¤ê³¼ í•¨ê»˜ ë†€ê³  ìˆë‹¤.",
            "lang": "ko",
            "method": "okt"
        },
        {
            "text": "The weather is beautiful today.",
            "lang": "en",
            "method": "okt"
        },
        {
            "text": "John and Mary eat an apple and a banana.",
            "lang": "en",
            "method": "okt"
        },
        {
            "text": "",  # ë¹ˆ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
            "lang": "ko",
            "method": "okt"
        }
    ]
    
    # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
    for i, test_case in enumerate(test_cases, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {test_case['text'][:30]}{'...' if len(test_case['text']) > 30 else ''}")
        print("-" * 40)
        
        try:
            result = extract_keywords(
                test_case['text'], 
                lang=test_case['lang'], 
                method=test_case['method']
            )
            
            # ê²°ê³¼ ì¶œë ¥
            if 'error' in result:
                print(f"âŒ ì˜¤ë¥˜: {result['error']}")
            else:
                print(f"âœ… ì–¸ì–´: {result['language']}")
                print(f"ğŸ“Š ë‹¨ì–´ ìˆ˜: {result['word_count']}")
                print(f"ğŸ”‘ ì¶”ì¶œ í‚¤ì›Œë“œ ìˆ˜: {result['keyword_count']}")
                print(f"ğŸ“ í‚¤ì›Œë“œ: {[kw['word'] for kw in result['keywords']]}")
                
        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
    
    print("\n" + "=" * 50)
    print("ğŸ¯ í‚¤ì›Œë“œ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ íƒœê±° í™•ì¸
    print("\nğŸ”§ ì‚¬ìš© ê°€ëŠ¥í•œ í•œêµ­ì–´ íƒœê±°:")
    try:
        extractor = KeywordExtractor("ko")
        available_taggers = list(extractor.ko_taggers.keys())
        if available_taggers:
            print(f"âœ… ì‚¬ìš© ê°€ëŠ¥: {', '.join(available_taggers)}")
        else:
            print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ íƒœê±°ê°€ ì—†ìŠµë‹ˆë‹¤. Java ì„¤ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        print(f"âŒ íƒœê±° í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
